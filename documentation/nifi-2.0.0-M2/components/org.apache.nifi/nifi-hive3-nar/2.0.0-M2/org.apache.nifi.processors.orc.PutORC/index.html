<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"></meta><title>PutORC</title><link rel="stylesheet" href="../../../../../css/component-usage.css" type="text/css"></link></head><script type="text/javascript">window.onload = function(){if(self==top) { document.getElementById('nameHeader').style.display = "inherit"; } }</script><body><h1 id="nameHeader" style="display: none;">PutORC</h1><h2>Description: </h2><p>Reads records from an incoming FlowFile using the provided Record Reader, and writes those records to a ORC file in the location/filesystem specified in the configuration.</p><h3>Tags: </h3><p>put, ORC, hadoop, HDFS, filesystem, restricted, record</p><h3>Properties: </h3><p>In the list below, the names of required properties appear in <strong>bold</strong>. Any other properties (not in bold) are considered optional. The table also indicates any default values, and whether a property supports the <a href="../../../../../html/expression-language-guide.html">NiFi Expression Language</a>.</p><table id="properties"><tr><th>Display Name</th><th>API Name</th><th>Default Value</th><th>Allowable Values</th><th>Description</th></tr><tr><td id="name">Hadoop Configuration Resources</td><td>Hadoop Configuration Resources</td><td></td><td id="allowable-values"></td><td id="description">A file or comma separated list of files which contains the Hadoop file system configuration. Without this, Hadoop will search the classpath for a 'core-site.xml' and 'hdfs-site.xml' file or will revert to a default configuration. To use swebhdfs, see 'Additional Details' section of PutHDFS's documentation.<br/><br/><strong>This property expects a comma-separated list of file resources.</strong><br/><br/><strong>Supports Expression Language: true (will be evaluated using Environment variables only)</strong></td></tr><tr><td id="name">Kerberos Credentials Service</td><td>kerberos-credentials-service</td><td></td><td id="allowable-values"><strong>Controller Service API: </strong><br/>KerberosCredentialsService<br/><strong>Implementation: </strong><a href="../../../nifi-kerberos-credentials-service-nar/2.0.0-M2/org.apache.nifi.kerberos.KeytabCredentialsService/index.html">KeytabCredentialsService</a></td><td id="description">Specifies the Kerberos Credentials Controller Service that should be used for authenticating with Kerberos</td></tr><tr><td id="name">Kerberos User Service</td><td>kerberos-user-service</td><td></td><td id="allowable-values"><strong>Controller Service API: </strong><br/>KerberosUserService<br/><strong>Implementations: </strong><a href="../../../nifi-kerberos-user-service-nar/2.0.0-M2/org.apache.nifi.kerberos.KerberosPasswordUserService/index.html">KerberosPasswordUserService</a><br/><a href="../../../nifi-kerberos-user-service-nar/2.0.0-M2/org.apache.nifi.kerberos.KerberosKeytabUserService/index.html">KerberosKeytabUserService</a><br/><a href="../../../nifi-kerberos-user-service-nar/2.0.0-M2/org.apache.nifi.kerberos.KerberosTicketCacheUserService/index.html">KerberosTicketCacheUserService</a></td><td id="description">Specifies the Kerberos User Controller Service that should be used for authenticating with Kerberos</td></tr><tr><td id="name">Kerberos Principal</td><td>Kerberos Principal</td><td></td><td id="allowable-values"></td><td id="description">Kerberos principal to authenticate as. Requires nifi.kerberos.krb5.file to be set in your nifi.properties<br/><strong>Supports Expression Language: true (will be evaluated using Environment variables only)</strong></td></tr><tr><td id="name">Kerberos Keytab</td><td>Kerberos Keytab</td><td></td><td id="allowable-values"></td><td id="description">Kerberos keytab associated with the principal. Requires nifi.kerberos.krb5.file to be set in your nifi.properties<br/><br/><strong>This property requires exactly one file to be provided..</strong><br/><br/><strong>Supports Expression Language: true (will be evaluated using Environment variables only)</strong></td></tr><tr><td id="name">Kerberos Password</td><td>Kerberos Password</td><td></td><td id="allowable-values"></td><td id="description">Kerberos password associated with the principal.<br/><strong>Sensitive Property: true</strong></td></tr><tr><td id="name">Kerberos Relogin Period</td><td>Kerberos Relogin Period</td><td id="default-value">4 hours</td><td id="allowable-values"></td><td id="description">Period of time which should pass before attempting a kerberos relogin.

This property has been deprecated, and has no effect on processing. Relogins now occur automatically.<br/><strong>Supports Expression Language: true (will be evaluated using Environment variables only)</strong></td></tr><tr><td id="name">Additional Classpath Resources</td><td>Additional Classpath Resources</td><td></td><td id="allowable-values"></td><td id="description">A comma-separated list of paths to files and/or directories that will be added to the classpath and used for loading native libraries. When specifying a directory, all files with in the directory will be added to the classpath, but further sub-directories will not be included.<br/><br/><strong>This property expects a comma-separated list of resources. Each of the resources may be of any of the following types: directory, file.</strong><br/></td></tr><tr><td id="name"><strong>Record Reader</strong></td><td>record-reader</td><td></td><td id="allowable-values"><strong>Controller Service API: </strong><br/>RecordReaderFactory<br/><strong>Implementations: </strong><a href="../../../nifi-record-serialization-services-nar/2.0.0-M2/org.apache.nifi.csv.CSVReader/index.html">CSVReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M2/org.apache.nifi.json.JsonPathReader/index.html">JsonPathReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M2/org.apache.nifi.avro.AvroReader/index.html">AvroReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M2/org.apache.nifi.cef.CEFReader/index.html">CEFReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M2/org.apache.nifi.syslog.Syslog5424Reader/index.html">Syslog5424Reader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M2/org.apache.nifi.json.JsonTreeReader/index.html">JsonTreeReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M2/org.apache.nifi.windowsevent.WindowsEventLogReader/index.html">WindowsEventLogReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M2/org.apache.nifi.xml.XMLReader/index.html">XMLReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M2/org.apache.nifi.syslog.SyslogReader/index.html">SyslogReader</a><br/><a href="../../../nifi-asn1-nar/2.0.0-M2/org.apache.nifi.jasn1.JASN1Reader/index.html">JASN1Reader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M2/org.apache.nifi.lookup.ReaderLookup/index.html">ReaderLookup</a><br/><a href="../../../nifi-parquet-nar/2.0.0-M2/org.apache.nifi.parquet.ParquetReader/index.html">ParquetReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M2/org.apache.nifi.grok.GrokReader/index.html">GrokReader</a><br/><a href="../../../nifi-scripting-nar/2.0.0-M2/org.apache.nifi.record.script.ScriptedReader/index.html">ScriptedReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M2/org.apache.nifi.yaml.YamlTreeReader/index.html">YamlTreeReader</a><br/><a href="../../../nifi-poi-nar/2.0.0-M2/org.apache.nifi.excel.ExcelReader/index.html">ExcelReader</a></td><td id="description">The service for reading records from incoming flow files.</td></tr><tr><td id="name"><strong>Directory</strong></td><td>Directory</td><td></td><td id="allowable-values"></td><td id="description">The parent directory to which files should be written. Will be created if it doesn't exist.<br/><strong>Supports Expression Language: true (will be evaluated using flow file attributes and Environment variables)</strong></td></tr><tr><td id="name"><strong>Compression Type</strong></td><td>compression-type</td><td id="default-value">NONE</td><td id="allowable-values"><ul><li>NONE <img src="../../../../../html/images/iconInfo.png" alt="No compression" title="No compression"></img></li><li>ZLIB <img src="../../../../../html/images/iconInfo.png" alt="ZLIB compression" title="ZLIB compression"></img></li><li>SNAPPY <img src="../../../../../html/images/iconInfo.png" alt="Snappy compression" title="Snappy compression"></img></li><li>LZO <img src="../../../../../html/images/iconInfo.png" alt="LZO compression" title="LZO compression"></img></li></ul></td><td id="description">The type of compression for the file being written.</td></tr><tr><td id="name"><strong>Overwrite Files</strong></td><td>overwrite</td><td id="default-value">false</td><td id="allowable-values"><ul><li>true</li><li>false</li></ul></td><td id="description">Whether or not to overwrite existing files in the same directory with the same name. When set to false, flow files will be routed to failure when a file exists in the same directory with the same name.</td></tr><tr><td id="name">Permissions umask</td><td>permissions-umask</td><td></td><td id="allowable-values"></td><td id="description">A umask represented as an octal number which determines the permissions of files written to HDFS. This overrides the Hadoop Configuration dfs.umaskmode</td></tr><tr><td id="name">Remote Group</td><td>remote-group</td><td></td><td id="allowable-values"></td><td id="description">Changes the group of the HDFS file to this value after it is written. This only works if NiFi is running as a user that has HDFS super user privilege to change group</td></tr><tr><td id="name">Remote Owner</td><td>remote-owner</td><td></td><td id="allowable-values"></td><td id="description">Changes the owner of the HDFS file to this value after it is written. This only works if NiFi is running as a user that has HDFS super user privilege to change owner</td></tr><tr><td id="name">ORC Configuration Resources</td><td>putorc-config-resources</td><td></td><td id="allowable-values"></td><td id="description">A file or comma separated list of files which contains the ORC configuration (hive-site.xml, e.g.). Without this, Hadoop will search the classpath for a 'hive-site.xml' file or will revert to a default configuration. Please see the ORC documentation for more details.<br/><br/><strong>This property expects a comma-separated list of file resources.</strong><br/></td></tr><tr><td id="name"><strong>Stripe Size</strong></td><td>putorc-stripe-size</td><td id="default-value">64 MB</td><td id="allowable-values"></td><td id="description">The size of the memory buffer (in bytes) for writing stripes to an ORC file</td></tr><tr><td id="name"><strong>Buffer Size</strong></td><td>putorc-buffer-size</td><td id="default-value">10 KB</td><td id="allowable-values"></td><td id="description">The maximum size of the memory buffers (in bytes) used for compressing and storing a stripe in memory. This is a hint to the ORC writer, which may choose to use a smaller buffer size based on stripe size and number of columns for efficient stripe writing and memory utilization.</td></tr><tr><td id="name">Hive Table Name</td><td>putorc-hive-table-name</td><td></td><td id="allowable-values"></td><td id="description">An optional table name to insert into the hive.ddl attribute. The generated DDL can be used by a PutHive3QL processor (presumably after a PutHDFS processor) to create a table backed by the converted ORC file. If this property is not provided, the full name (including namespace) of the incoming Avro record will be normalized and used as the table name.<br/><strong>Supports Expression Language: true (will be evaluated using flow file attributes and Environment variables)</strong></td></tr><tr><td id="name"><strong>Normalize Field Names for Hive</strong></td><td>putorc-hive-field-names</td><td id="default-value">true</td><td id="allowable-values"><ul><li>true</li><li>false</li></ul></td><td id="description">Whether to normalize field names for Hive (force lowercase, e.g.). If the ORC file is going to be part of a Hive table, this property should be set to true. To preserve the original field names from the schema, this property should be set to false.</td></tr></table><h3>Relationships: </h3><table id="relationships"><tr><th>Name</th><th>Description</th></tr><tr><td>retry</td><td>Flow Files that could not be processed due to issues that can be retried are transferred to this relationship</td></tr><tr><td>success</td><td>Flow Files that have been successfully processed are transferred to this relationship</td></tr><tr><td>failure</td><td>Flow Files that could not be processed due to issue that cannot be retried are transferred to this relationship</td></tr></table><h3>Reads Attributes: </h3><table id="reads-attributes"><tr><th>Name</th><th>Description</th></tr><tr><td>filename</td><td>The name of the file to write comes from the value of this attribute.</td></tr></table><h3>Writes Attributes: </h3><table id="writes-attributes"><tr><th>Name</th><th>Description</th></tr><tr><td>filename</td><td>The name of the file is stored in this attribute.</td></tr><tr><td>absolute.hdfs.path</td><td>The absolute path to the file is stored in this attribute.</td></tr><tr><td>hadoop.file.url</td><td>The hadoop url for the file is stored in this attribute.</td></tr><tr><td>record.count</td><td>The number of records written to the ORC file</td></tr><tr><td>hive.ddl</td><td>Creates a partial Hive DDL statement for creating an external table in Hive from the destination folder. This can be used in ReplaceText for setting the content to the DDL. To make it valid DDL, add "LOCATION '&lt;path_to_orc_file_in_hdfs&gt;'", where the path is the directory that contains this ORC file on HDFS. For example, this processor can send flow files downstream to ReplaceText to set the content to this DDL (plus the LOCATION clause as described), then to PutHiveQL processor to create the table if it doesn't exist.</td></tr></table><h3>State management: </h3>This component does not store state.<h3>Restricted: </h3><table id="restrictions"><tr><th>Required Permission</th><th>Explanation</th></tr><tr><td>write distributed filesystem</td><td>Provides operator the ability to write to any file that NiFi has access to in HDFS or the local filesystem.</td></tr></table><h3>Input requirement: </h3>This component requires an incoming relationship.<h3>System Resource Considerations:</h3>None specified.</body></html>