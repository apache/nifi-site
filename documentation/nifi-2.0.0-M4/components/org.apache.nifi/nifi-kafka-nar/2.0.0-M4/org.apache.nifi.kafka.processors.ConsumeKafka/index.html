<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"></meta><title>ConsumeKafka</title><link rel="stylesheet" href="../../../../../css/component-usage.css" type="text/css"></link></head><script type="text/javascript">window.onload = function(){if(self==top) { document.getElementById('nameHeader').style.display = "inherit"; } }</script><body><h1 id="nameHeader" style="display: none;">ConsumeKafka</h1><h2>Description: </h2><p>Consumes messages from Apache Kafka Consumer API. The complementary NiFi processor for sending messages is PublishKafka. The Processor supports consumption of Kafka messages, optionally interpreted as NiFi records. Please note that, at this time (in read record mode), the Processor assumes that all records that are retrieved from a given partition have the same schema. For this mode, if any of the Kafka messages are pulled but cannot be parsed or written with the configured Record Reader or Record Writer, the contents of the message will be written to a separate FlowFile, and that FlowFile will be transferred to the 'parse.failure' relationship. Otherwise, each FlowFile is sent to the 'success' relationship and may contain many individual messages within the single FlowFile. A 'record.count' attribute is added to indicate how many messages are contained in the FlowFile. No two Kafka messages will be placed into the same FlowFile if they have different schemas, or if they have different values for a message header that is included by the &lt;Headers to Add as Attributes&gt; property.</p><h3>Tags: </h3><p>Kafka, Get, Record, csv, avro, json, Ingest, Ingress, Topic, PubSub, Consume</p><h3>Properties: </h3><p>In the list below, the names of required properties appear in <strong>bold</strong>. Any other properties (not in bold) are considered optional. The table also indicates any default values, and whether a property supports the <a href="../../../../../html/expression-language-guide.html">NiFi Expression Language</a>.</p><table id="properties"><tr><th>Display Name</th><th>API Name</th><th>Default Value</th><th>Allowable Values</th><th>Description</th></tr><tr><td id="name"><strong>Kafka Connection Service</strong></td><td>Kafka Connection Service</td><td></td><td id="allowable-values"><strong>Controller Service API: </strong><br/>KafkaConnectionService<br/><strong>Implementation: </strong><a href="../../../nifi-kafka-3-service-nar/2.0.0-M4/org.apache.nifi.kafka.service.Kafka3ConnectionService/index.html">Kafka3ConnectionService</a></td><td id="description">Provides connections to Kafka Broker for publishing Kafka Records</td></tr><tr><td id="name"><strong>Group ID</strong></td><td>Group ID</td><td></td><td id="allowable-values"></td><td id="description">Kafka Consumer Group Identifier corresponding to Kafka group.id property</td></tr><tr><td id="name"><strong>Topic Format</strong></td><td>Topic Format</td><td id="default-value">names</td><td id="allowable-values"><ul><li>names <img src="../../../../../html/images/iconInfo.png" alt="Topic is a full topic name or comma separated list of names" title="Topic is a full topic name or comma separated list of names"></img></li><li>pattern <img src="../../../../../html/images/iconInfo.png" alt="Topic is a regular expression according to the Java Pattern syntax" title="Topic is a regular expression according to the Java Pattern syntax"></img></li></ul></td><td id="description">Specifies whether the Topics provided are a comma separated list of names or a single regular expression</td></tr><tr><td id="name"><strong>Topics</strong></td><td>Topics</td><td></td><td id="allowable-values"></td><td id="description">The name or pattern of the Kafka Topics from which the Processor consumes Kafka Records. More than one can be supplied if comma separated.<br/><strong>Supports Expression Language: true (will be evaluated using Environment variables only)</strong></td></tr><tr><td id="name"><strong>Auto Offset Reset</strong></td><td>auto.offset.reset</td><td id="default-value">latest</td><td id="allowable-values"><ul><li>earliest <img src="../../../../../html/images/iconInfo.png" alt="Automatically reset the offset to the earliest offset" title="Automatically reset the offset to the earliest offset"></img></li><li>latest <img src="../../../../../html/images/iconInfo.png" alt="Automatically reset the offset to the latest offset" title="Automatically reset the offset to the latest offset"></img></li><li>none <img src="../../../../../html/images/iconInfo.png" alt="Throw exception to the consumer if no previous offset found for the consumer group" title="Throw exception to the consumer if no previous offset found for the consumer group"></img></li></ul></td><td id="description">Automatic offset configuration applied when no previous consumer offset found corresponding to Kafka auto.offset.reset property</td></tr><tr><td id="name"><strong>Commit Offsets</strong></td><td>Commit Offsets</td><td id="default-value">true</td><td id="allowable-values"><ul><li>true</li><li>false</li></ul></td><td id="description">Specifies whether this Processor should commit the offsets to Kafka after receiving messages. Typically, this value should be set to true so that messages that are received are not duplicated. However, in certain scenarios, we may want to avoid committing the offsets, that the data can be processed and later acknowledged by PublishKafka in order to provide Exactly Once semantics.</td></tr><tr><td id="name"><strong>Max Uncommitted Time</strong></td><td>Max Uncommitted Time</td><td id="default-value">1 s</td><td id="allowable-values"></td><td id="description">Specifies the maximum amount of time allowed to pass before offsets must be committed. This value impacts how often offsets will be committed. Committing offsets less often increases throughput but also increases the window of potential data duplication in the event of a rebalance or JVM restart between commits. This value is also related to maximum poll records and the use of a message demarcator. When using a message demarcator we can have far more uncommitted messages than when we're not as there is much less for us to keep track of in memory.<br/><br/><strong>This Property is only considered if </strong><strong>the [Commit Offsets] Property has a value of "true".</strong></td></tr><tr><td id="name">Header Name Pattern</td><td>Header Name Pattern</td><td></td><td id="allowable-values"></td><td id="description">Regular Expression Pattern applied to Kafka Record Header Names for selecting Header Values to be written as FlowFile attributes</td></tr><tr><td id="name"><strong>Header Encoding</strong></td><td>Header Encoding</td><td id="default-value">UTF-8</td><td id="allowable-values"></td><td id="description">Character encoding applied when reading Kafka Record Header values and writing FlowFile attributes</td></tr><tr><td id="name"><strong>Processing Strategy</strong></td><td>Processing Strategy</td><td id="default-value">FLOW_FILE</td><td id="allowable-values"><ul><li>FLOW_FILE <img src="../../../../../html/images/iconInfo.png" alt="Write one FlowFile for each Kafka Record consumed" title="Write one FlowFile for each Kafka Record consumed"></img></li><li>DEMARCATOR <img src="../../../../../html/images/iconInfo.png" alt="Write one FlowFile for each batch of Kafka Records consumed (optionally grouped by Kafka key)" title="Write one FlowFile for each batch of Kafka Records consumed (optionally grouped by Kafka key)"></img></li><li>RECORD <img src="../../../../../html/images/iconInfo.png" alt="Write one FlowFile containing multiple Kafka Records consumed and processed with Record Reader and Record Writer" title="Write one FlowFile containing multiple Kafka Records consumed and processed with Record Reader and Record Writer"></img></li></ul></td><td id="description">Strategy for processing Kafka Records and writing serialized output to FlowFiles</td></tr><tr><td id="name"><strong>Record Reader</strong></td><td>Record Reader</td><td></td><td id="allowable-values"><strong>Controller Service API: </strong><br/>RecordReaderFactory<br/><strong>Implementations: </strong><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.json.JsonPathReader/index.html">JsonPathReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.syslog.Syslog5424Reader/index.html">Syslog5424Reader</a><br/><a href="../../../nifi-scripting-nar/2.0.0-M4/org.apache.nifi.record.script.ScriptedReader/index.html">ScriptedReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.cef.CEFReader/index.html">CEFReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.avro.AvroReader/index.html">AvroReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.csv.CSVReader/index.html">CSVReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.json.JsonTreeReader/index.html">JsonTreeReader</a><br/><a href="../../../nifi-protobuf-services-nar/2.0.0-M4/org.apache.nifi.services.protobuf.ProtobufReader/index.html">ProtobufReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.windowsevent.WindowsEventLogReader/index.html">WindowsEventLogReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.syslog.SyslogReader/index.html">SyslogReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.xml.XMLReader/index.html">XMLReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.lookup.ReaderLookup/index.html">ReaderLookup</a><br/><a href="../../../nifi-poi-nar/2.0.0-M4/org.apache.nifi.excel.ExcelReader/index.html">ExcelReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.yaml.YamlTreeReader/index.html">YamlTreeReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.grok.GrokReader/index.html">GrokReader</a></td><td id="description">The Record Reader to use for incoming Kafka messages<br/><br/><strong>This Property is only considered if </strong><strong>the [Processing Strategy] Property has a value of "RECORD".</strong></td></tr><tr><td id="name"><strong>Record Writer</strong></td><td>Record Writer</td><td></td><td id="allowable-values"><strong>Controller Service API: </strong><br/>RecordSetWriterFactory<br/><strong>Implementations: </strong><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.lookup.RecordSetWriterLookup/index.html">RecordSetWriterLookup</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.avro.AvroRecordSetWriter/index.html">AvroRecordSetWriter</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.text.FreeFormTextRecordSetWriter/index.html">FreeFormTextRecordSetWriter</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.json.JsonRecordSetWriter/index.html">JsonRecordSetWriter</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.xml.XMLRecordSetWriter/index.html">XMLRecordSetWriter</a><br/><a href="../../../nifi-scripting-nar/2.0.0-M4/org.apache.nifi.record.script.ScriptedRecordSetWriter/index.html">ScriptedRecordSetWriter</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.csv.CSVRecordSetWriter/index.html">CSVRecordSetWriter</a></td><td id="description">The Record Writer to use in order to serialize the outgoing FlowFiles<br/><br/><strong>This Property is only considered if </strong><strong>the [Processing Strategy] Property has a value of "RECORD".</strong></td></tr><tr><td id="name"><strong>Output Strategy</strong></td><td>Output Strategy</td><td id="default-value">Use Content as Value</td><td id="allowable-values"><ul><li>Use Content as Value <img src="../../../../../html/images/iconInfo.png" alt="Write only the Kafka Record value to the FlowFile record." title="Write only the Kafka Record value to the FlowFile record."></img></li><li>Use Wrapper <img src="../../../../../html/images/iconInfo.png" alt="Write the Kafka Record key, value, headers, and metadata into the FlowFile record. (See processor usage for more information.)" title="Write the Kafka Record key, value, headers, and metadata into the FlowFile record. (See processor usage for more information.)"></img></li></ul></td><td id="description">The format used to output the Kafka Record into a FlowFile Record.<br/><br/><strong>This Property is only considered if </strong><strong>the [Processing Strategy] Property has a value of "RECORD".</strong></td></tr><tr><td id="name"><strong>Key Attribute Encoding</strong></td><td>Key Attribute Encoding</td><td id="default-value">UTF-8 Encoded</td><td id="allowable-values"><ul><li>UTF-8 Encoded <img src="../../../../../html/images/iconInfo.png" alt="The key is interpreted as a UTF-8 Encoded string." title="The key is interpreted as a UTF-8 Encoded string."></img></li><li>Hex Encoded <img src="../../../../../html/images/iconInfo.png" alt="The key is interpreted as arbitrary binary data and is encoded using hexadecimal characters with uppercase letters" title="The key is interpreted as arbitrary binary data and is encoded using hexadecimal characters with uppercase letters"></img></li><li>Do Not Add Key as Attribute <img src="../../../../../html/images/iconInfo.png" alt="The key will not be added as an Attribute" title="The key will not be added as an Attribute"></img></li></ul></td><td id="description">Encoding for value of configured FlowFile attribute containing Kafka Record Key.<br/><br/><strong>This Property is only considered if </strong><strong>the [Output Strategy] Property has a value of "Use Content as Value".</strong></td></tr><tr><td id="name"><strong>Key Format</strong></td><td>Key Format</td><td id="default-value">Byte Array</td><td id="allowable-values"><ul><li>String <img src="../../../../../html/images/iconInfo.png" alt="Format the Kafka ConsumerRecord key as a UTF-8 string." title="Format the Kafka ConsumerRecord key as a UTF-8 string."></img></li><li>Byte Array <img src="../../../../../html/images/iconInfo.png" alt="Format the Kafka ConsumerRecord key as a byte array." title="Format the Kafka ConsumerRecord key as a byte array."></img></li><li>Record <img src="../../../../../html/images/iconInfo.png" alt="Format the Kafka ConsumerRecord key as a record." title="Format the Kafka ConsumerRecord key as a record."></img></li></ul></td><td id="description">Specifies how to represent the Kafka Record Key in the output FlowFile<br/><br/><strong>This Property is only considered if </strong><strong>the [Output Strategy] Property has a value of "Use Wrapper".</strong></td></tr><tr><td id="name"><strong>Key Record Reader</strong></td><td>Key Record Reader</td><td></td><td id="allowable-values"><strong>Controller Service API: </strong><br/>RecordReaderFactory<br/><strong>Implementations: </strong><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.json.JsonPathReader/index.html">JsonPathReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.syslog.Syslog5424Reader/index.html">Syslog5424Reader</a><br/><a href="../../../nifi-scripting-nar/2.0.0-M4/org.apache.nifi.record.script.ScriptedReader/index.html">ScriptedReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.cef.CEFReader/index.html">CEFReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.avro.AvroReader/index.html">AvroReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.csv.CSVReader/index.html">CSVReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.json.JsonTreeReader/index.html">JsonTreeReader</a><br/><a href="../../../nifi-protobuf-services-nar/2.0.0-M4/org.apache.nifi.services.protobuf.ProtobufReader/index.html">ProtobufReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.windowsevent.WindowsEventLogReader/index.html">WindowsEventLogReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.syslog.SyslogReader/index.html">SyslogReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.xml.XMLReader/index.html">XMLReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.lookup.ReaderLookup/index.html">ReaderLookup</a><br/><a href="../../../nifi-poi-nar/2.0.0-M4/org.apache.nifi.excel.ExcelReader/index.html">ExcelReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.yaml.YamlTreeReader/index.html">YamlTreeReader</a><br/><a href="../../../nifi-record-serialization-services-nar/2.0.0-M4/org.apache.nifi.grok.GrokReader/index.html">GrokReader</a></td><td id="description">The Record Reader to use for parsing the Kafka Record Key into a Record<br/><br/><strong>This Property is only considered if </strong><strong>the [Key Format] Property has a value of "Record".</strong></td></tr><tr><td id="name"><strong>Message Demarcator</strong></td><td>Message Demarcator</td><td></td><td id="allowable-values"></td><td id="description">Since KafkaConsumer receives messages in batches, this Processor has an option to output FlowFiles which contains all Kafka messages in a single batch for a given topic and partition and this property allows you to provide a string (interpreted as UTF-8) to use for demarcating apart multiple Kafka messages. This is an optional property and if not provided each Kafka message received will result in a single FlowFile which  time it is triggered. To enter special character such as 'new line' use CTRL+Enter or Shift+Enter depending on the OS<br/><br/><strong>This Property is only considered if </strong><strong>the [Processing Strategy] Property has a value of "DEMARCATOR".</strong></td></tr><tr><td id="name"><strong>Separate By Key</strong></td><td>Separate By Key</td><td id="default-value">false</td><td id="allowable-values"><ul><li>true</li><li>false</li></ul></td><td id="description">When this property is enabled, two messages will only be added to the same FlowFile if both of the Kafka Messages have identical keys.<br/><br/><strong>This Property is only considered if </strong><strong>the [Message Demarcator] Property has a value specified.</strong></td></tr></table><h3>Relationships: </h3><table id="relationships"><tr><th>Name</th><th>Description</th></tr><tr><td>success</td><td>FlowFiles containing one or more serialized Kafka Records</td></tr></table><h3>Reads Attributes: </h3>None specified.<h3>Writes Attributes: </h3><table id="writes-attributes"><tr><th>Name</th><th>Description</th></tr><tr><td>record.count</td><td>The number of records received</td></tr><tr><td>mime.type</td><td>The MIME Type that is provided by the configured Record Writer</td></tr><tr><td>kafka.count</td><td>The number of messages written if more than one</td></tr><tr><td>kafka.key</td><td>The key of message if present and if single message. How the key is encoded depends on the value of the 'Key Attribute Encoding' property.</td></tr><tr><td>kafka.offset</td><td>The offset of the message in the partition of the topic.</td></tr><tr><td>kafka.timestamp</td><td>The timestamp of the message in the partition of the topic.</td></tr><tr><td>kafka.partition</td><td>The partition of the topic the message or message bundle is from</td></tr><tr><td>kafka.topic</td><td>The topic the message or message bundle is from</td></tr><tr><td>kafka.tombstone</td><td>Set to true if the consumed message is a tombstone message</td></tr></table><h3>State management: </h3>This component does not store state.<h3>Restricted: </h3>This component is not restricted.<h3>Input requirement: </h3>This component does not allow an incoming relationship.<h3>System Resource Considerations:</h3>None specified.<h3>See Also:</h3><p><a href="../org.apache.nifi.kafka.processors.PublishKafka/index.html">PublishKafka</a></p></body></html>